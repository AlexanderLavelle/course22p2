{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "import pickle,gzip,math,os,time,shutil,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "# from torch import tensor,nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from jax.scipy import special\n",
    "from optax import losses\n",
    "\n",
    "from cytoolz import partition_all\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:14:25.591740: W external/xla/xla/service/gpu/nvptx_compiler.cc:718] The NVIDIA driver's CUDA version is 12.3 which is older than the ptxas CUDA version (12.4.131). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "from fastcore.test import test_close\n",
    "\n",
    "# torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "# torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "# x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n",
    "x_train, y_train, x_valid, y_valid = map(jnp.array, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):   \n",
    "    n_in: int\n",
    "    nh: int\n",
    "    n_out: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [nn.Dense(self.nh), nn.relu, nn.Dense(self.n_out)]\n",
    "\n",
    "    \n",
    "    # def __init__(self, n_in, nh, n_out):\n",
    "    #     super().__init__()\n",
    "        # self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
    "\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, n_out=10)\n",
    "key = jax.random.key(42)\n",
    "params = model.init(key, x_train)\n",
    "pred = model.apply(params, x_train)\n",
    "# pred = model(x_train)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$ \n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n",
    "def log_softmax(x): return jnp.log((jnp.exp(x)/(jnp.sum(jnp.exp(x), axis=-1, keepdims=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.7616212 -2.4488273 -2.3964708 ... -2.1585255 -2.4422033 -2.0649405]\n",
      " [-2.6426332 -2.229197  -2.151103  ... -2.5405574 -2.2871442 -2.5302484]\n",
      " [-2.2792363 -2.4974546 -2.168021  ... -2.482406  -2.3646762 -2.6438835]\n",
      " ...\n",
      " [-2.2595613 -2.4760604 -2.346009  ... -2.1632507 -2.2752864 -2.2968078]\n",
      " [-2.531205  -2.5598724 -2.1569514 ... -2.232909  -2.396523  -2.3275523]\n",
      " [-2.3378534 -2.5673983 -2.0652215 ... -2.4250243 -2.3522823 -2.3984969]]\n"
     ]
    }
   ],
   "source": [
    "smx = log_softmax(pred)\n",
    "print(smx)\n",
    "del smx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\n",
    "def log_softmax(x): return x - jnp.log(jnp.sum(jnp.exp(x), axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = jnp.max(x, axis=-1)\n",
    "    # return m + (x-m[:,None]).exp().sum(-1).log()\n",
    "    return m + jnp.log(jnp.sum(jnp.exp(x-m[:,None]), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - special.logsumexp(x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-2.7616215, -2.4488275, -2.3964708, ..., -2.1585257, -2.4422035,\n",
       "        -2.0649405],\n",
       "       [-2.642633 , -2.2291968, -2.1511028, ..., -2.5405571, -2.2871437,\n",
       "        -2.5302482],\n",
       "       [-2.2792363, -2.4974546, -2.168021 , ..., -2.482406 , -2.3646762,\n",
       "        -2.6438832],\n",
       "       ...,\n",
       "       [-2.2595615, -2.4760606, -2.3460093, ..., -2.1632507, -2.2752864,\n",
       "        -2.2968078],\n",
       "       [-2.531205 , -2.5598726, -2.1569514, ..., -2.232909 , -2.3965232,\n",
       "        -2.3275523],\n",
       "       [-2.3378534, -2.5673983, -2.0652215, ..., -2.4250243, -2.3522823,\n",
       "        -2.3984969]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_close(logsumexp(pred), special.logsumexp(pred, axis=-1))\n",
    "sm_pred = log_softmax(pred)\n",
    "sm_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "But since our $x$s are 1-hot encoded (actually, they're just the integer indices), this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target.\n",
    "\n",
    "This can be done using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([5, 0, 4], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(-2.2299705, dtype=float32),\n",
       " Array(-2.642633, dtype=float32),\n",
       " Array(-2.2255213, dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[0,5],sm_pred[1,0],sm_pred[2,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-2.2299705, -2.642633 , -2.2255213], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0,1,2], y_train[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nll(input, target): return -input[range(target.shape[0]), target].mean()\n",
    "def nll(input, target): return -jnp.mean(input.at[:target.shape[0], target].get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm_pred.at(range(y_train.shape[0]), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-2.2299705, -2.7616215, -2.1726105, ..., -2.4422035, -2.1726105,\n",
       "        -2.4422035],\n",
       "       [-2.3071775, -2.642633 , -1.9782999, ..., -2.2871437, -1.9782999,\n",
       "        -2.2871437]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred.at[:2, y_train].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9d42cad89d4901a71fa36103b321e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(2.3271935, dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss = nll(sm_pred, y_train)\n",
    "# loss\n",
    "\n",
    "loss = []\n",
    "for p, true in tqdm(zip(partition_all(5_000, sm_pred), partition_all(5_000, y_train))):\n",
    "    loss.append(nll(jnp.stack(p), jnp.stack(true)))\n",
    "    del p, true\n",
    "loss = jnp.mean(jnp.array(loss))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use PyTorch's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.3554947, dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(losses.softmax_cross_entropy_with_integer_labels(pred, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optax import softmax_cross_entropy_with_integer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.3554947, dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(softmax_cross_entropy_with_integer_labels(pred, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(2.3554947, dtype=float32), Array(2.3271935, dtype=float32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(softmax_cross_entropy_with_integer_labels(pred, y_train)), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "close:\n2.355494737625122\n2.32719349861145",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test_close(F.cross_entropy(pred, y_train), loss, 1e-3)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoftmax_cross_entropy_with_integer_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge-pypy3/envs/fastai_jax/lib/python3.10/site-packages/fastcore/test.py:63\u001b[0m, in \u001b[0;36mtest_close\u001b[0;34m(a, b, eps)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_close\u001b[39m(a,b,eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`test` that `a` is within `eps` of `b`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_close\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge-pypy3/envs/fastai_jax/lib/python3.10/site-packages/fastcore/test.py:27\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`assert` that `cmp(a,b)`; display inputs and `cname or cmp.__name__` if it fails\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: cname\u001b[38;5;241m=\u001b[39mcmp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m cmp(a,b),\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: close:\n2.355494737625122\n2.32719349861145"
     ]
    }
   ],
   "source": [
    "# test_close(F.cross_entropy(pred, y_train), loss, 1e-3)\n",
    "test_close(jnp.mean(softmax_cross_entropy_with_integer_labels(pred, y_train)), loss, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the training loop repeats over the following steps:\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = F.cross_entropy\n",
    "loss_func = softmax_cross_entropy_with_integer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-0.45370492, -0.14091104, -0.08855445,  0.03231215,  0.1353058 ,\n",
       "         0.07794579,  0.06304041,  0.14939064, -0.1342871 ,  0.2429759 ],      dtype=float32),\n",
       " (50, 10))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=50                  # batch size\n",
    "\n",
    "xb = x_train[0:bs]     # a mini-batch from x\n",
    "# preds = model(xb)      # predictions\n",
    "preds = model.apply(params, xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0,\n",
       "       9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n",
       "       3, 9, 8, 5, 9, 3], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2.2299705, 2.642633 , 2.2255213, 2.576247 , 2.4146516, 2.225658 ,\n",
       "       2.3225238, 2.356748 , 2.3506737, 2.0716798, 2.2000601, 2.2572718,\n",
       "       2.0863912, 2.7246022, 2.4060407, 2.5091004, 2.2461243, 2.4645007,\n",
       "       2.6515172, 2.3453064, 2.33957  , 2.8971386, 2.3907444, 2.5407145,\n",
       "       2.55032  , 2.1285377, 2.2201312, 2.1550813, 2.4150817, 2.2604208,\n",
       "       2.3592186, 2.2590723, 2.573631 , 2.2131736, 2.7940185, 2.0177536,\n",
       "       2.5907516, 2.8610444, 2.2017884, 2.7423017, 2.3397846, 2.3395984,\n",
       "       2.586595 , 2.192431 , 2.2054746, 2.3544729, 2.3593862, 2.0810244,\n",
       "       2.228127 , 2.3452876], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.376998, dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([9, 4, 5, 4, 4, 4, 7, 4, 5, 5, 2, 7, 2, 4, 2, 5, 5, 4, 5, 2, 5, 4,\n",
       "       4, 4, 5, 4, 5, 9, 3, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 5, 7, 5, 4,\n",
       "       4, 4, 2, 5, 7, 9], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds.argmax(dim=1)\n",
    "jnp.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# def accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()\n",
    "def accuracy(out, yb): return jnp.mean(jnp.argmax(out, axis=1) == yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.04, dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5   # learning rate\n",
    "epochs = 3 # how many epochs to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.38, 0.04\n"
     ]
    }
   ],
   "source": [
    "xb,yb = x_train[:bs],y_train[:bs]\n",
    "# preds = model(xb)\n",
    "preds = model.apply(params, xb)\n",
    "# report(loss_func(preds, yb), preds, yb)\n",
    "report(jnp.mean(loss_func(preds, yb)), preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss(model, params, inputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_fn(pred, true):\n",
    "    return jnp.mean(loss_func(pred, true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cytoolz import curry\n",
    "get_grads = jax.grad(l_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.00126378,  0.0017279 ,  0.00182077,  0.0020547 ,  0.0022776 ,\n",
       "        -0.01784937,  0.00211881,  0.00230991,  0.00173938,  0.00253652],\n",
       "       [-0.01857653,  0.0021523 ,  0.00232712,  0.00190433,  0.00276608,\n",
       "         0.00199084,  0.00223551,  0.00157645,  0.00203112,  0.00159279],\n",
       "       [ 0.00204725,  0.00164588,  0.00228808,  0.00219581, -0.01783978,\n",
       "         0.00287849,  0.00181215,  0.00167084,  0.00187959,  0.00142169],\n",
       "       [ 0.00195763, -0.01847882,  0.00207379,  0.00193786,  0.00257795,\n",
       "         0.0016266 ,  0.00231283,  0.00221074,  0.00166783,  0.00211359],\n",
       "       [ 0.00178956,  0.00164998,  0.0021288 ,  0.00173376,  0.00247667,\n",
       "         0.00235406,  0.00236586,  0.00183212,  0.00188125, -0.01821203],\n",
       "       [ 0.00146661,  0.00127838, -0.01784007,  0.00187929,  0.00284169,\n",
       "         0.00204141,  0.00232544,  0.00229887,  0.0017451 ,  0.00196328],\n",
       "       [ 0.00138673, -0.01803948,  0.0023345 ,  0.00165702,  0.00226941,\n",
       "         0.00233998,  0.00189701,  0.0023853 ,  0.00174313,  0.0020264 ],\n",
       "       [ 0.0014394 ,  0.00144633,  0.0020231 , -0.01810544,  0.00278386,\n",
       "         0.00175561,  0.00203155,  0.00242139,  0.00185351,  0.00235071],\n",
       "       [ 0.0014849 , -0.0180939 ,  0.00240093,  0.00171467,  0.00211147,\n",
       "         0.00248285,  0.00194345,  0.00203042,  0.00198726,  0.00193796],\n",
       "       [ 0.00178951,  0.00140583,  0.0017312 ,  0.00191445, -0.01748052,\n",
       "         0.0030712 ,  0.00182698,  0.0018715 ,  0.002365  ,  0.00150484],\n",
       "       [ 0.00133376,  0.00173847,  0.00256807, -0.01778407,  0.00253873,\n",
       "         0.00186256,  0.00191529,  0.00188692,  0.00202444,  0.00191582],\n",
       "       [ 0.00188826,  0.00173019,  0.00193248,  0.00215434,  0.00192667,\n",
       "        -0.01790729,  0.0021681 ,  0.00217545,  0.00193212,  0.00199967],\n",
       "       [ 0.00164674,  0.00165971,  0.00289222, -0.01751731,  0.00163613,\n",
       "         0.00218122,  0.00213735,  0.00159015,  0.00184263,  0.00193115],\n",
       "       [ 0.00137   ,  0.00169754,  0.0021095 ,  0.00190746,  0.00395379,\n",
       "         0.00272201, -0.01868855,  0.00193753,  0.00162841,  0.00136232],\n",
       "       [ 0.00178189, -0.01819657,  0.00229682,  0.00164082,  0.00224035,\n",
       "         0.00216838,  0.00194275,  0.00227185,  0.0017411 ,  0.00211261],\n",
       "       [ 0.0016885 ,  0.00161925,  0.00231733,  0.00173743,  0.00215819,\n",
       "         0.00274145,  0.00219785, -0.01837317,  0.00209595,  0.00181724],\n",
       "       [ 0.0012592 ,  0.00205847, -0.01788383,  0.00185106,  0.00228591,\n",
       "         0.00231465,  0.0022791 ,  0.00192436,  0.00213812,  0.00177297],\n",
       "       [ 0.00196969,  0.00150597,  0.00222362,  0.00154854,  0.00278196,\n",
       "         0.00224128,  0.00197539,  0.00212045, -0.01829897,  0.00193208],\n",
       "       [ 0.00170076,  0.0019001 ,  0.00251046,  0.00188547,  0.00247426,\n",
       "         0.00289427, -0.01858912,  0.00190006,  0.00172934,  0.0015944 ],\n",
       "       [ 0.00167673,  0.00175318,  0.00259645,  0.00179485,  0.00232636,\n",
       "         0.00239504,  0.00192908,  0.00168632,  0.00192564, -0.01808364],\n",
       "       [ 0.00151046,  0.00234363,  0.00247609,  0.00139106, -0.01807262,\n",
       "         0.00266181,  0.0020557 ,  0.00243316,  0.00148519,  0.00171552],\n",
       "       [-0.01889638,  0.00197562,  0.00237812,  0.00178031,  0.00267576,\n",
       "         0.00233213,  0.00244666,  0.00142939,  0.00214519,  0.00173321],\n",
       "       [ 0.0014714 ,  0.0018774 ,  0.00225712,  0.00181015,  0.00274016,\n",
       "         0.00229815,  0.00210335,  0.00175876,  0.00185228, -0.01816877],\n",
       "       [ 0.00189248, -0.0184238 ,  0.00208834,  0.00204768,  0.00253758,\n",
       "         0.00177788,  0.00227095,  0.00214598,  0.00167691,  0.001986  ],\n",
       "       [ 0.00126012, -0.01843887,  0.00221377,  0.00185582,  0.00281606,\n",
       "         0.00291201,  0.00193934,  0.00174034,  0.00193407,  0.00176734],\n",
       "       [ 0.00183703,  0.00126297, -0.01761978,  0.00125207,  0.00251704,\n",
       "         0.00225389,  0.00247523,  0.0024792 ,  0.00168292,  0.00185943],\n",
       "       [ 0.00149235,  0.00187584,  0.00214307,  0.00225896, -0.0178281 ,\n",
       "         0.00235891,  0.00183204,  0.00177114,  0.00220016,  0.00189563],\n",
       "       [ 0.00179193,  0.00103675,  0.00233483, -0.01768212,  0.00202736,\n",
       "         0.00161016,  0.00220893,  0.00205421,  0.0017964 ,  0.00282157],\n",
       "       [ 0.00186672,  0.00169064, -0.0182128 ,  0.00231204,  0.0022888 ,\n",
       "         0.00200034,  0.00225459,  0.00206759,  0.00210739,  0.00162468],\n",
       "       [ 0.00164887,  0.00151142,  0.00233467,  0.00190539,  0.00250095,\n",
       "         0.0023707 ,  0.00189956, -0.01791387,  0.00189504,  0.00184728],\n",
       "       [ 0.00157818,  0.00170782,  0.00218666, -0.01811012,  0.00217484,\n",
       "         0.00259079,  0.00173317,  0.0025651 ,  0.00166082,  0.00191275],\n",
       "       [ 0.00151583,  0.00129646,  0.00226406,  0.00159008,  0.00267668,\n",
       "         0.00201576,  0.00214875,  0.00212625, -0.01791105,  0.00227719],\n",
       "       [ 0.00169495,  0.00176054,  0.00222845,  0.00202492,  0.00231344,\n",
       "         0.00228921, -0.01847484,  0.00223128,  0.00199079,  0.00194127],\n",
       "       [ 0.00174822,  0.00134281,  0.00224161,  0.00219197,  0.00214176,\n",
       "         0.00244852,  0.00208063,  0.00194317,  0.00167424, -0.01781294],\n",
       "       [-0.0187765 ,  0.0015561 ,  0.0021213 ,  0.00163778,  0.00308027,\n",
       "         0.0028263 ,  0.00147491,  0.00238131,  0.00192971,  0.00176883],\n",
       "       [ 0.00160938,  0.00161817,  0.00185788,  0.00174521,  0.00189172,\n",
       "        -0.01734092,  0.00221194,  0.00226961,  0.00195557,  0.00218145],\n",
       "       [ 0.00108944,  0.00164995,  0.00206282,  0.00173997,  0.00264082,\n",
       "         0.00341093, -0.01850073,  0.00202432,  0.00181657,  0.00206591],\n",
       "       [-0.01885582,  0.00156174,  0.00238245,  0.00157692,  0.00365088,\n",
       "         0.00238775,  0.00175723,  0.0017842 ,  0.00186462,  0.00189001],\n",
       "       [ 0.00133591,  0.00187506,  0.00195898,  0.00203699,  0.00248279,\n",
       "         0.00218712,  0.00215019, -0.0177879 ,  0.00172428,  0.00203659],\n",
       "       [ 0.00150611,  0.00160524,  0.00231628,  0.00166603,  0.00303592,\n",
       "         0.0029214 , -0.01871156,  0.00228134,  0.00184402,  0.00153523],\n",
       "       [ 0.00139962, -0.01807303,  0.00215331,  0.00199315,  0.00234663,\n",
       "         0.00248457,  0.00167557,  0.001852  ,  0.00226829,  0.00189989],\n",
       "       [ 0.00144706,  0.00166001,  0.00201445,  0.00195207,  0.00236817,\n",
       "         0.00223344,  0.00178069,  0.002457  , -0.01807267,  0.00215977],\n",
       "       [ 0.0014235 ,  0.00208836,  0.00212968,  0.00185927,  0.00211741,\n",
       "         0.00274859,  0.00200828, -0.01849448,  0.00247986,  0.00163953],\n",
       "       [ 0.00159341,  0.00172982,  0.00216001,  0.00212266,  0.0022634 ,\n",
       "         0.00186806,  0.00212251,  0.00196982,  0.00193742, -0.0177671 ],\n",
       "       [ 0.00170244,  0.00190622,  0.00220038, -0.01779604,  0.00221918,\n",
       "         0.00185627,  0.00198197,  0.00199391,  0.00191303,  0.00202264],\n",
       "       [ 0.00145432,  0.00207128,  0.00194271,  0.0018055 ,  0.00255594,\n",
       "         0.00216936,  0.0021655 ,  0.00202505,  0.00191147, -0.01810113],\n",
       "       [ 0.00151365,  0.00160394,  0.00281501,  0.00210784,  0.00258305,\n",
       "         0.0017563 ,  0.0017596 ,  0.0019378 , -0.01811044,  0.00203324],\n",
       "       [ 0.00175255,  0.00194618,  0.00192415,  0.00190191,  0.0023201 ,\n",
       "        -0.01750395,  0.00190805,  0.00180664,  0.0020902 ,  0.00185417],\n",
       "       [ 0.00174694,  0.00197259,  0.00216469,  0.00171301,  0.00212072,\n",
       "         0.00200447,  0.00188929,  0.0022163 ,  0.00201737, -0.0178454 ],\n",
       "       [ 0.00138258,  0.00127895,  0.0021738 , -0.01808361,  0.00220797,\n",
       "         0.00168919,  0.00236629,  0.00240263,  0.00184764,  0.00273455]],      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_grads(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     for i in range(0, n, bs):\n",
    "#         s = slice(i, min(n,i+bs))\n",
    "#         xb,yb = x_train[s],y_train[s]\n",
    "#         preds = model(xb)\n",
    "#         loss = loss_func(preds, yb)\n",
    "\n",
    "#         # have to do backward pass\n",
    "#         loss.backward()\n",
    "#         with torch.no_grad():\n",
    "#             for l in model.layers:\n",
    "#                 if hasattr(l, 'weight'):\n",
    "#                     l.weight -= l.weight.grad * lr\n",
    "#                     l.bias   -= l.bias.grad   * lr\n",
    "#                     l.weight.grad.zero_()\n",
    "#                     l.bias  .grad.zero_()\n",
    "#     report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20, 0.86\n",
      "0.18, 0.94\n",
      "0.14, 0.92\n"
     ]
    }
   ],
   "source": [
    "def get_loss_model(model, params, features, y_true):\n",
    "    preds = model.apply(params, features)\n",
    "    loss = jnp.mean(loss_func(preds, y_true))\n",
    "    return loss, preds\n",
    "\n",
    "glm = curry(get_loss_model, model)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n,i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        \n",
    "        # (loss, preds), grads = jax.value_and_grad(glm, argnums=0, has_aux=True)(params, xb, yb)  \n",
    "        (loss, preds), grads = jax.value_and_grad(glm, argnums=0, has_aux=True)(params, xb, yb)     \n",
    "\n",
    "\n",
    "        # grads are equivalent to loss.backward\n",
    "        for layer_key in grads['params'].keys():\n",
    "            for k,v in grads['params'][layer_key].items():\n",
    "                params['params'][layer_key][k] -= v * lr\n",
    "                \n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using parameters and optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1 = nn.Module()\n",
    "# m1.foo = nn.Linear(3,4)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    inp: int\n",
    "    opt: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.m1 = nn.Dense(self.opt)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.m1(x)\n",
    "        \n",
    "m1 = Model(3, 4)\n",
    "key = jax.random.key(42)\n",
    "params = model.init(key, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                 Model Summary                                  \u001b[0m\n",
      "┏━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mflops \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mvjp_flops\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams     \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
      "│      │ Model  │ \u001b[2mfloat32\u001b[0m[50… │ \u001b[2mfloat32\u001b[0m[50… │ 313800 │ 944336    │             │\n",
      "├──────┼────────┼─────────────┼─────────────┼────────┼───────────┼─────────────┤\n",
      "│ m1   │ Dense  │ \u001b[2mfloat32\u001b[0m[50… │ \u001b[2mfloat32\u001b[0m[50… │ 313800 │ 944336    │ bias:       │\n",
      "│      │        │             │             │        │           │ \u001b[2mfloat32\u001b[0m[4]  │\n",
      "│      │        │             │             │        │           │ kernel:     │\n",
      "│      │        │             │             │        │           │ \u001b[2mfloat32\u001b[0m[78… │\n",
      "│      │        │             │             │        │           │             │\n",
      "│      │        │             │             │        │           │ \u001b[1m3,140 \u001b[0m\u001b[1;2m(12.6\u001b[0m │\n",
      "│      │        │             │             │        │           │ \u001b[1;2mKB)\u001b[0m         │\n",
      "├──────┼────────┼─────────────┼─────────────┼────────┼───────────┼─────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m    Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3,140 \u001b[0m\u001b[1;2m(12.6\u001b[0m\u001b[1m \u001b[0m│\n",
      "│\u001b[1m      \u001b[0m│\u001b[1m        \u001b[0m│\u001b[1m             \u001b[0m│\u001b[1m             \u001b[0m│\u001b[1m        \u001b[0m│\u001b[1m           \u001b[0m│\u001b[1m \u001b[0m\u001b[1;2mKB)\u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m│\n",
      "└──────┴────────┴─────────────┴─────────────┴────────┴───────────┴─────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                       Total Parameters: 3,140 \u001b[0m\u001b[1;2m(12.6 KB)\u001b[0m\u001b[1m                        \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(m1.tabulate(key, xb, compute_flops=True, compute_vjp_flops=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(m1.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1.named_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'layers_0': {'kernel': Array([[ 0.00084758,  0.06064403, -0.06266869, ..., -0.00681735,\n",
       "           -0.00187519, -0.02079083],\n",
       "          [-0.01123159,  0.00575195,  0.03618244, ...,  0.01979761,\n",
       "           -0.01443758, -0.00126234],\n",
       "          [ 0.00869545, -0.00858875,  0.06376506, ..., -0.01492844,\n",
       "           -0.0450354 ,  0.03631166],\n",
       "          ...,\n",
       "          [-0.03845134, -0.00953184, -0.04398863, ...,  0.02352371,\n",
       "            0.06378859, -0.0080379 ],\n",
       "          [ 0.04453897, -0.01387933, -0.03332501, ..., -0.07309227,\n",
       "            0.02147802,  0.01484199],\n",
       "          [ 0.01643012, -0.03393842,  0.05047164, ...,  0.01506289,\n",
       "            0.07461423, -0.01509868]], dtype=float32),\n",
       "   'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)},\n",
       "  'layers_2': {'kernel': Array([[ 1.35197401e-01, -1.60713926e-01,  1.50378302e-01,\n",
       "            1.11816049e-01, -9.69363376e-02, -1.10430062e-01,\n",
       "            1.39277294e-01, -2.26301402e-01,  8.13967139e-02,\n",
       "           -1.48562029e-01],\n",
       "          [-3.90475653e-02, -2.30567694e-01,  1.05523773e-01,\n",
       "            1.44236341e-01, -2.00501174e-01, -1.01782382e-01,\n",
       "           -1.98010042e-01, -6.00512661e-02, -4.27698195e-02,\n",
       "           -5.15916795e-02],\n",
       "          [ 5.03115952e-02,  1.69857696e-03,  1.06750518e-01,\n",
       "           -1.44835800e-01, -1.15045525e-01,  1.21765703e-01,\n",
       "            7.29507431e-02, -1.32856756e-01,  8.83537903e-02,\n",
       "           -1.84778646e-01],\n",
       "          [ 2.02200606e-01, -5.33163324e-02,  2.37359360e-01,\n",
       "           -6.60006925e-02,  6.40383065e-02,  6.20770864e-02,\n",
       "           -1.50420964e-01, -2.49802798e-01, -1.92229852e-01,\n",
       "           -3.11297148e-01],\n",
       "          [ 9.68085229e-02,  1.59481138e-01,  1.54630989e-01,\n",
       "           -9.72549021e-02, -2.74178058e-01,  2.63953537e-01,\n",
       "            1.55419901e-01,  1.61911726e-01,  7.19902746e-05,\n",
       "           -1.66267715e-02],\n",
       "          [-1.41913131e-01, -2.86162615e-01, -3.96430418e-02,\n",
       "           -1.09781213e-01,  1.28180236e-01,  2.61855066e-01,\n",
       "           -1.55589893e-01, -1.24357946e-01,  1.45018011e-01,\n",
       "           -2.94520825e-01],\n",
       "          [ 9.98082459e-02, -1.86811492e-01, -1.08311094e-01,\n",
       "            6.18980415e-02, -1.07995950e-01, -5.17759398e-02,\n",
       "           -1.28834724e-01,  1.20908603e-01, -1.97436839e-01,\n",
       "           -7.41318008e-03],\n",
       "          [ 3.55964005e-02, -1.55675307e-01,  9.42170843e-02,\n",
       "           -2.80093968e-01, -4.62166667e-02, -9.10832584e-02,\n",
       "            1.76527306e-01, -8.60637948e-02, -1.87992275e-01,\n",
       "           -1.26670584e-01],\n",
       "          [ 2.93097403e-02, -1.48112988e-02,  2.48826784e-03,\n",
       "            1.04465686e-01,  1.97940156e-01, -1.23335440e-02,\n",
       "           -6.82831034e-02, -6.19830415e-02,  1.25529900e-01,\n",
       "           -1.38818668e-02],\n",
       "          [ 2.54686296e-01,  1.13153018e-01, -1.42451718e-01,\n",
       "            2.49646511e-02,  2.37566024e-01, -1.97369784e-01,\n",
       "           -2.53796913e-02,  3.16743068e-02,  5.70782274e-02,\n",
       "            3.09097171e-01],\n",
       "          [-3.94673571e-02,  4.70074899e-02, -8.82572830e-02,\n",
       "           -1.98010042e-01, -1.58254415e-01,  9.09514353e-02,\n",
       "            3.51273012e-03, -9.12047401e-02, -5.39413728e-02,\n",
       "            4.74445568e-03],\n",
       "          [ 2.67954975e-01, -1.34654984e-01,  8.42784345e-02,\n",
       "           -1.13973819e-01, -1.01125427e-01, -1.06510535e-01,\n",
       "           -9.19635296e-02,  1.63071811e-01,  4.76328135e-02,\n",
       "            2.46823147e-01],\n",
       "          [ 8.10292289e-02,  2.08823726e-01,  1.63159102e-01,\n",
       "           -1.19003072e-01, -8.88346359e-02, -7.74594918e-02,\n",
       "            2.75158000e-05, -4.75259311e-02, -5.29815350e-03,\n",
       "           -4.45272736e-02],\n",
       "          [ 1.70244090e-02, -1.21159241e-01,  2.26919904e-01,\n",
       "            3.17342192e-01,  7.07291067e-02,  2.39720762e-01,\n",
       "           -1.73537761e-01,  1.77625448e-01,  1.69498902e-02,\n",
       "           -2.39099972e-02],\n",
       "          [-4.03214544e-02, -1.48249418e-01,  8.99543762e-02,\n",
       "           -2.73248944e-02, -6.02085292e-02, -8.63137245e-02,\n",
       "            1.98294029e-01,  3.07774842e-01, -2.09840015e-01,\n",
       "           -3.09440583e-01],\n",
       "          [-2.28634953e-01, -7.18129948e-02, -2.18988821e-01,\n",
       "           -2.03046232e-01,  2.36779734e-01, -1.21842086e-01,\n",
       "            1.10676125e-01,  2.41118237e-01,  5.59064299e-02,\n",
       "           -7.85686597e-02],\n",
       "          [ 3.98856886e-02, -7.39377588e-02, -2.11636186e-01,\n",
       "            8.37813132e-03, -2.36585274e-01, -1.87659547e-01,\n",
       "            9.82606485e-02,  1.58635333e-01, -2.28978693e-01,\n",
       "           -1.83824629e-01],\n",
       "          [ 1.18676208e-01, -3.03423792e-01,  1.39084548e-01,\n",
       "           -5.77010494e-03, -2.92086210e-02,  2.47269928e-01,\n",
       "            1.62091523e-01,  2.06223622e-01,  1.15886204e-01,\n",
       "           -1.36767194e-01],\n",
       "          [-2.93868594e-02, -1.63710445e-01,  2.26838350e-01,\n",
       "            1.14143729e-01,  7.15053305e-02, -1.40320510e-01,\n",
       "            9.17886663e-03,  1.27741709e-01, -3.67454998e-03,\n",
       "            1.45595163e-01],\n",
       "          [-1.13067292e-02,  3.43058407e-02, -1.57383159e-01,\n",
       "            5.75368805e-03,  1.10109016e-01, -1.86472945e-02,\n",
       "            2.41873115e-01,  1.06240399e-01,  1.03642292e-01,\n",
       "            2.44836941e-01],\n",
       "          [-1.08792529e-01,  7.36391172e-02, -1.17986619e-01,\n",
       "           -4.18754891e-02,  1.27878860e-01, -4.11882065e-02,\n",
       "           -1.42391011e-01, -1.26958052e-02, -3.10000062e-01,\n",
       "            1.88942313e-01],\n",
       "          [-5.01553118e-02, -7.56696099e-03,  1.06129870e-01,\n",
       "           -1.99581347e-02,  1.04412466e-01,  3.19768369e-01,\n",
       "           -3.20801616e-01,  6.82968572e-02,  2.12562475e-02,\n",
       "           -8.13842714e-02],\n",
       "          [-2.72671223e-01,  8.56985971e-02,  1.09030299e-01,\n",
       "           -1.45896256e-01,  3.10700536e-02,  5.75381666e-02,\n",
       "            2.32246727e-01, -8.38031545e-02,  4.22510281e-02,\n",
       "            1.32370993e-01],\n",
       "          [ 1.22607209e-01,  1.26925141e-01,  6.23322874e-02,\n",
       "            3.46159302e-02,  2.16849446e-01, -1.16857491e-01,\n",
       "            1.12289779e-01,  2.64265865e-01, -7.43675083e-02,\n",
       "           -2.49932781e-02],\n",
       "          [ 2.35941187e-02, -3.77074294e-02, -1.63531322e-02,\n",
       "           -1.58699462e-03,  1.11707628e-01, -3.19876075e-01,\n",
       "            2.88172960e-01, -7.68881366e-02, -2.25784406e-02,\n",
       "            1.21420965e-01],\n",
       "          [ 1.34666443e-01, -4.12150426e-03,  1.00955099e-01,\n",
       "            1.55111149e-01,  6.06181994e-02,  1.38961658e-01,\n",
       "            1.61603123e-01, -1.43046319e-01, -8.19105208e-02,\n",
       "            2.30388083e-02],\n",
       "          [ 1.35276586e-01,  8.66531134e-02, -6.28252178e-02,\n",
       "            6.35716543e-02, -3.05495769e-01,  2.48744301e-02,\n",
       "            3.19181412e-01, -9.07533243e-02,  1.15366848e-02,\n",
       "           -8.36582705e-02],\n",
       "          [-1.67724878e-01, -1.96017660e-02,  7.45356642e-03,\n",
       "           -2.36424237e-01, -7.73732811e-02,  1.77696392e-01,\n",
       "           -7.13306293e-02, -1.22617632e-01, -1.16417684e-01,\n",
       "           -1.59860905e-02],\n",
       "          [ 4.04983535e-02, -8.99008363e-02,  4.55857515e-02,\n",
       "            3.93886901e-02,  1.93883240e-01, -2.75322527e-01,\n",
       "            7.63657466e-02,  1.58401504e-01, -1.76252812e-01,\n",
       "            1.80100828e-01],\n",
       "          [ 6.74554184e-02,  1.36880830e-01,  1.67606562e-01,\n",
       "           -1.83377057e-01,  3.82345393e-02, -2.22492218e-02,\n",
       "            3.11041512e-02,  8.20381120e-02, -1.99037418e-02,\n",
       "           -1.30823538e-01],\n",
       "          [ 1.96599662e-02, -3.20262685e-02, -6.88206553e-02,\n",
       "            2.32384130e-01, -2.04425976e-01,  8.45840946e-02,\n",
       "           -7.23777562e-02,  1.20753467e-01, -2.15085372e-01,\n",
       "            2.81843692e-01],\n",
       "          [-1.02264814e-01, -1.29599944e-01,  1.69154286e-01,\n",
       "           -1.49000868e-01,  2.46113073e-02, -2.58443683e-01,\n",
       "           -1.27346246e-02,  3.42285857e-02, -4.03776243e-02,\n",
       "            2.72683203e-01],\n",
       "          [ 8.24976042e-02,  2.50928074e-01,  6.04530945e-02,\n",
       "            1.94017008e-01,  1.79911837e-01,  1.46885529e-01,\n",
       "            1.49024472e-01,  4.75932844e-02, -1.95453972e-01,\n",
       "            8.95057693e-02],\n",
       "          [ 4.85608242e-02,  1.78840160e-01, -1.84974790e-01,\n",
       "            1.70830101e-01,  9.44449753e-02, -6.23154417e-02,\n",
       "           -1.29349902e-01,  1.53598651e-01,  1.02368400e-01,\n",
       "           -1.14676282e-02],\n",
       "          [-3.10881287e-01, -1.58681870e-01,  1.08103871e-01,\n",
       "           -1.26140311e-01,  1.38405934e-01,  2.30907928e-02,\n",
       "           -1.37099609e-01, -1.87947482e-01,  7.31995329e-02,\n",
       "           -6.09352402e-02],\n",
       "          [-1.92331016e-01,  8.25924054e-02, -2.57825315e-01,\n",
       "            5.84651902e-02,  8.22790265e-02, -7.03940028e-03,\n",
       "            1.28444225e-01,  1.86771620e-02,  2.48379171e-01,\n",
       "           -1.90172926e-01],\n",
       "          [-2.16449589e-01, -7.22527578e-02, -2.65992999e-01,\n",
       "            4.15641256e-02,  1.39241204e-01, -5.53481281e-02,\n",
       "           -1.31203562e-01, -4.66825347e-03, -1.52334869e-01,\n",
       "           -4.72632460e-02],\n",
       "          [-5.82163818e-02,  9.74562764e-02,  9.75612737e-03,\n",
       "           -1.10536672e-01, -3.71559002e-02, -1.36200711e-01,\n",
       "            1.48428470e-01, -1.51399434e-01, -4.64232974e-02,\n",
       "            1.72365606e-01],\n",
       "          [-1.36350766e-01,  6.59340844e-02,  1.89702556e-01,\n",
       "            1.26904070e-01,  2.03431234e-01,  1.24170706e-01,\n",
       "           -1.50075376e-01, -1.67597324e-01, -6.99087977e-02,\n",
       "           -5.96143752e-02],\n",
       "          [-5.06465994e-02, -1.92043893e-02,  3.25622447e-02,\n",
       "           -3.47187221e-02, -2.11692706e-01,  2.19165772e-01,\n",
       "            6.45957217e-02,  1.39273137e-01, -2.13256538e-01,\n",
       "            3.48754115e-02],\n",
       "          [-1.50262430e-01,  1.75395533e-01, -1.28384084e-02,\n",
       "           -1.04740404e-01,  1.07508320e-02,  2.59507418e-01,\n",
       "           -2.14575324e-02,  2.26033211e-01,  4.69199531e-02,\n",
       "            2.46016577e-01],\n",
       "          [-2.10011125e-01, -4.30906191e-02,  1.11681938e-01,\n",
       "           -1.14418738e-01,  1.84158623e-01,  1.49396837e-01,\n",
       "            5.30693531e-02,  2.16049641e-01,  7.95141086e-02,\n",
       "            4.28616628e-02],\n",
       "          [-1.91330910e-01, -3.12810168e-02, -3.28244865e-02,\n",
       "           -8.98916721e-02,  1.85749188e-01, -2.46946022e-01,\n",
       "           -1.81193769e-01,  1.21249065e-01, -4.90925536e-02,\n",
       "           -8.57068524e-02],\n",
       "          [ 8.48196819e-02, -1.76378354e-01,  2.20577046e-01,\n",
       "            2.91488111e-01,  1.31492704e-01, -8.15432295e-02,\n",
       "            9.80694219e-02, -1.67107835e-01,  2.00711843e-02,\n",
       "            8.05258006e-02],\n",
       "          [-1.58810824e-01,  4.22901399e-02,  1.88581198e-01,\n",
       "            8.29514787e-02,  9.80642363e-02,  1.69878021e-01,\n",
       "            3.63189052e-03, -3.21440965e-01,  1.32895168e-02,\n",
       "           -1.77416742e-01],\n",
       "          [-2.16519549e-01,  2.16962785e-01, -9.38082263e-02,\n",
       "            6.15615994e-02, -1.72051683e-01,  2.79581517e-01,\n",
       "           -4.18798476e-02, -2.32856274e-01,  2.27667242e-01,\n",
       "           -1.65020123e-01],\n",
       "          [ 2.58351237e-01, -1.67475566e-01,  2.21310854e-02,\n",
       "           -2.87198603e-01,  1.27557606e-01,  1.42610475e-01,\n",
       "            2.73752455e-02,  7.94472843e-02, -1.79026812e-01,\n",
       "            2.63725910e-02],\n",
       "          [-2.64044076e-01,  2.19685528e-02,  1.08106405e-01,\n",
       "            2.27487683e-01,  3.94030958e-02, -7.56683946e-02,\n",
       "           -2.91608959e-01, -9.54191163e-02,  5.69329076e-02,\n",
       "           -9.43413600e-02],\n",
       "          [ 8.96075815e-02,  8.81204847e-03, -1.24434724e-01,\n",
       "            8.14511403e-02,  1.93728656e-01,  9.53432843e-02,\n",
       "            2.56409440e-02, -2.60384381e-02, -8.99741575e-02,\n",
       "            1.36006698e-01],\n",
       "          [ 7.24631324e-02, -2.53657877e-01, -4.39147986e-02,\n",
       "            1.84151202e-01, -6.37267977e-02,  2.66236216e-01,\n",
       "            2.27798283e-01,  6.03341348e-02,  2.08016828e-01,\n",
       "            1.42748386e-01]], dtype=float32),\n",
       "   'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)}}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(m1.parameters())\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    n_in: int\n",
    "    nh: int\n",
    "    n_out: int\n",
    "    \n",
    "    # def __init__(self, n_in, nh, n_out):\n",
    "    #     super().__init__()\n",
    "    #     self.l1 = nn.Linear(n_in,nh)\n",
    "    #     self.l2 = nn.Linear(nh,n_out)\n",
    "    #     self.relu = nn.ReLU()\n",
    "\n",
    "    def setup(self):\n",
    "        self.l1 = nn.Dense(self.nh)\n",
    "        self.l2 = nn.Dense(self.n_out)\n",
    "        # self.relu = flax.linen.relu()\n",
    "        \n",
    "    # def forward(self, x): return self.l2(self.relu(self.l1(x)))\n",
    "    def __call__(self, x): return self.l2(nn.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(m, nh, 10)\n",
    "params = model.init(jax.random.key(42), x_train)\n",
    "# model.l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "    # attributes\n",
       "    n_in = 784\n",
       "    nh = 50\n",
       "    n_out = 10\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                  MLP Summary                                   \u001b[0m\n",
      "┏━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mflops  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mvjp_flops\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams     \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
      "│      │ MLP    │ \u001b[2mfloat32\u001b[0m[50… │ \u001b[2mfloat32\u001b[0m[5… │ 3975500 │ 11963200  │             │\n",
      "├──────┼────────┼─────────────┼────────────┼─────────┼───────────┼─────────────┤\n",
      "│ l1   │ Dense  │ \u001b[2mfloat32\u001b[0m[50… │ \u001b[2mfloat32\u001b[0m[5… │ 3922500 │ 11804200  │ bias:       │\n",
      "│      │        │             │            │         │           │ \u001b[2mfloat32\u001b[0m[50] │\n",
      "│      │        │             │            │         │           │ kernel:     │\n",
      "│      │        │             │            │         │           │ \u001b[2mfloat32\u001b[0m[78… │\n",
      "│      │        │             │            │         │           │             │\n",
      "│      │        │             │            │         │           │ \u001b[1m39,250 \u001b[0m     │\n",
      "│      │        │             │            │         │           │ \u001b[1;2m(157.0 KB)\u001b[0m  │\n",
      "├──────┼────────┼─────────────┼────────────┼─────────┼───────────┼─────────────┤\n",
      "│ l2   │ Dense  │ \u001b[2mfloat32\u001b[0m[50… │ \u001b[2mfloat32\u001b[0m[5… │ 50500   │ 151500    │ bias:       │\n",
      "│      │        │             │            │         │           │ \u001b[2mfloat32\u001b[0m[10] │\n",
      "│      │        │             │            │         │           │ kernel:     │\n",
      "│      │        │             │            │         │           │ \u001b[2mfloat32\u001b[0m[50… │\n",
      "│      │        │             │            │         │           │             │\n",
      "│      │        │             │            │         │           │ \u001b[1m510 \u001b[0m\u001b[1;2m(2.0 \u001b[0m   │\n",
      "│      │        │             │            │         │           │ \u001b[1;2mKB)\u001b[0m         │\n",
      "├──────┼────────┼─────────────┼────────────┼─────────┼───────────┼─────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m    Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m39,760     \u001b[0m\u001b[1m \u001b[0m│\n",
      "│\u001b[1m      \u001b[0m│\u001b[1m        \u001b[0m│\u001b[1m             \u001b[0m│\u001b[1m            \u001b[0m│\u001b[1m         \u001b[0m│\u001b[1m           \u001b[0m│\u001b[1m \u001b[0m\u001b[1;2m(159.0 KB)\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m│\n",
      "└──────┴────────┴─────────────┴────────────┴─────────┴───────────┴─────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                      Total Parameters: 39,760 \u001b[0m\u001b[1;2m(159.0 KB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for name,l in model.named_children(): print(f\"{name}: {l}\")\n",
    "print(model.tabulate(key, xb, compute_flops=True, compute_vjp_flops=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'l1': {'bias': (50,), 'kernel': (784, 50)},\n",
       "  'l2': {'bias': (10,), 'kernel': (50, 10)}}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for p in model.parameters(): print(p.shape)\n",
    "jax.tree_util.tree_map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(jax_fw_pass, params):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, n, bs):\n",
    "            s = slice(i, min(n,i+bs))\n",
    "            xb,yb = x_train[s],y_train[s]\n",
    "            \n",
    "        #     preds = model(xb)\n",
    "        #     loss = loss_func(preds, yb)\n",
    "        #     loss.backward()\n",
    "        #     with torch.no_grad():\n",
    "        #         for p in model.parameters(): p -= p.grad * lr\n",
    "        #         model.zero_grad()\n",
    "        # report(loss, preds, yb)\n",
    "\n",
    "        (loss, preds), grads = jax.value_and_grad(jax_fw_pass, argnums=0, has_aux=True)(params, xb, yb)     \n",
    "\n",
    "\n",
    "        # grads are equivalent to loss.backward\n",
    "        for layer_key in grads['params'].keys():\n",
    "            for k,v in grads['params'][layer_key].items():\n",
    "                params['params'][layer_key][k] -= v * lr\n",
    "                    \n",
    "        report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_fw_pass = curry(get_loss_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.35, 0.08\n",
      "1.97, 0.42\n",
      "1.60, 0.78\n"
     ]
    }
   ],
   "source": [
    "# fit()\n",
    "fit(jax_fw_pass, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model.\n",
    "\n",
    "### In JAX this isn't strictly true since parameters exist outside the model and are therefore not registered into the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModule:\n",
    "#     def __init__(self, n_in, nh, n_out):\n",
    "#         self._modules = {}\n",
    "#         self.l1 = nn.Linear(n_in,nh)\n",
    "#         self.l2 = nn.Linear(nh,n_out)\n",
    "\n",
    "#     def __setattr__(self,k,v):\n",
    "#         if not k.startswith(\"_\"): self._modules[k] = v\n",
    "#         super().__setattr__(k,v)\n",
    "\n",
    "#     def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "#     def parameters(self):\n",
    "#         for l in self._modules.values(): yield from l.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl = MyModule(m,nh,10)\n",
    "# mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in mdl.parameters(): print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the original `layers` approach, but we have to register the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]\n",
    "layers = [nn.Dense(nh), nn.relu, nn.Dense(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cytoolz import thread_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self, layers):\n",
    "#         super().__init__()\n",
    "#         self.layers = layers\n",
    "#         for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "\n",
    "#     def forward(self, x): return reduce(lambda val,layer: layer(val), self.layers, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(layers)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.ModuleList` does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    inp_layers: list\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = self.inp_layers\n",
    "        \n",
    "    # def __init__(self, layers):\n",
    "    #     super().__init__()\n",
    "    #     self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    # def forward(self, x):\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SequentialModel(nn.Module):\n",
    "#     layers: list\n",
    "        \n",
    "#     def __call__(self, x):\n",
    "#         for l in self.layers: x = l(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "    # attributes\n",
       "    inp_layers = [Dense(\n",
       "        # attributes\n",
       "        features = 50\n",
       "        use_bias = True\n",
       "        dtype = None\n",
       "        param_dtype = float32\n",
       "        precision = None\n",
       "        kernel_init = init\n",
       "        bias_init = zeros\n",
       "        dot_general = None\n",
       "        dot_general_cls = None\n",
       "    ), <jax._src.custom_derivatives.custom_jvp object at 0x74a59d64ec20>, Dense(\n",
       "        # attributes\n",
       "        features = 10\n",
       "        use_bias = True\n",
       "        dtype = None\n",
       "        param_dtype = float32\n",
       "        precision = None\n",
       "        kernel_init = init\n",
       "        bias_init = zeros\n",
       "        dot_general = None\n",
       "        dot_general_cls = None\n",
       "    )]\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialModel(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init(jax.random.key(42), x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_fw_pass = curry(get_loss_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.32, 0.10\n",
      "1.88, 0.50\n",
      "1.52, 0.68\n"
     ]
    }
   ],
   "source": [
    "fit(jax_fw_pass, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` is a convenient class which does the same as the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    nh: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return thread_first(\n",
    "            x,\n",
    "            nn.Dense(nh), \n",
    "            nn.relu, \n",
    "            nn.Dense(10),\n",
    "        )\n",
    "\n",
    "model = Model(nh)\n",
    "params = model.init(jax.random.key(42), xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.32, 0.08\n",
      "1.96, 0.52\n",
      "1.58, 0.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array([0.84741366, 0.74000174, 0.5388808 , 0.36210084, 0.8273415 ,\n",
       "        1.6482549 , 0.711874  , 1.0429876 , 1.8101919 , 0.74176687,\n",
       "        1.8879099 , 2.257093  , 0.56879216, 1.844775  , 2.263823  ,\n",
       "        0.564728  , 1.6968277 , 1.3402851 , 1.765891  , 0.58509743,\n",
       "        1.036983  , 0.4104269 , 2.1872613 , 0.99904   , 0.71518445,\n",
       "        0.7481323 , 0.5691101 , 0.70057935, 1.572373  , 1.8575803 ,\n",
       "        1.5375795 , 1.4306394 , 1.2342105 , 1.1522068 , 0.8590653 ,\n",
       "        1.725537  , 1.8949713 , 1.4486787 , 1.3742487 , 0.79983455,\n",
       "        1.8367733 , 0.922026  , 1.1009822 , 0.6674258 , 1.2506173 ,\n",
       "        0.66259664, 0.8287465 , 2.1102612 , 1.5048342 , 1.70374   ],      dtype=float32),\n",
       " Array(0.7, dtype=float32))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit()\n",
    "jax_fw_pass = curry(get_loss_model, model)\n",
    "fit(jax_fw_pass, params)\n",
    "# loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss_func(model.apply(params, xb), yb), accuracy(model.apply(params, xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    # def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
    "    def __init__(self, params, lr=0.5): self.params,self.lr=params,lr\n",
    "\n",
    "\n",
    "    def step(self, grads):\n",
    "        # with torch.no_grad():\n",
    "        #     for p in self.params: p -= p.grad * self.lr\n",
    "\n",
    "        # grads are equivalent to loss.backward\n",
    "        for layer_key in grads['params'].keys():\n",
    "            for k,v in grads['params'][layer_key].items():\n",
    "                self.params['params'][layer_key][k] -= v * lr\n",
    "\n",
    "    # no need to zero grads\n",
    "    # def zero_grad(self):\n",
    "    #     for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = Optimizer(model.parameters())\n",
    "opt = Optimizer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16, 0.98\n",
      "0.11, 0.98\n",
      "0.09, 0.96\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n,i+bs))\n",
    "        xb,yb = x_train[s],y_train[s]\n",
    "\n",
    "        # preds = model(xb)\n",
    "        # loss = loss_func(preds, yb)\n",
    "        # loss.backward()\n",
    "        (loss, preds), grads = jax.value_and_grad(jax_fw_pass, argnums=0, has_aux=True)(params, xb, yb)     \n",
    "\n",
    "        # opt.step()\n",
    "        opt.step(grads)\n",
    "        # opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model():\n",
    "#     model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
    "#     return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def get_model(nh=nh, xb=xb, lr=lr):\n",
    "    model = model = Model(nh)\n",
    "    params = model.init(jax.random.key(42), xb)\n",
    "    opt = optax.sgd(learning_rate=lr)\n",
    "    opt_state = opt.init(params)\n",
    "    return (model, params), (opt, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model,opt = get_model()\n",
    "# loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     for i in range(0, n, bs):\n",
    "#         s = slice(i, min(n,i+bs))\n",
    "#         xb,yb = x_train[s],y_train[s]\n",
    "#         preds = model(xb)\n",
    "#         loss = loss_func(preds, yb)\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         opt.zero_grad()\n",
    "#     report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optax.sgd(learning_rate=lr)\n",
    "opt_state = opt.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06, 0.98\n",
      "0.05, 0.98\n",
      "0.04, 0.98\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n,i+bs))\n",
    "        xb,yb = x_train[s],y_train[s]\n",
    "        (loss, preds), grads = jax.value_and_grad(jax_fw_pass, argnums=0, has_aux=True)(params, xb, yb)     \n",
    "        updates, opt_state = opt.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clunky to iterate through minibatches of x and y values separately:\n",
    "\n",
    "```python\n",
    "    xb = x_train[s]\n",
    "    yb = y_train[s]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[s]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "assert len(train_ds)==len(x_train)\n",
    "assert len(valid_ds)==len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " Array([5, 0, 4, 1, 9], dtype=int32))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15, 0.96\n",
      "0.08, 0.98\n",
      "0.09, 0.96\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     for i in range(0, n, bs):\n",
    "#         xb,yb = train_ds[i:min(n,i+bs)]\n",
    "#         preds = model(xb)\n",
    "#         loss = loss_func(preds, yb)\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         opt.zero_grad()\n",
    "#     report(loss, preds, yb)\n",
    "\n",
    "model, params = model\n",
    "opt, opt_state = opt\n",
    "jax_fw_pass = jax_fw_pass = curry(get_loss_model, model)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n,i+bs))\n",
    "        xb,yb = x_train[s],y_train[s]\n",
    "        (loss, preds), grads = jax.value_and_grad(jax_fw_pass, argnums=0, has_aux=True)(params, xb, yb)     \n",
    "        updates, opt_state = opt.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "\n",
    "```python\n",
    "for i in range(0, n, bs):\n",
    "    xb,yb = train_ds[i:min(n,i+bs)]\n",
    "    ...\n",
    "```\n",
    "\n",
    "Let's make our loop much cleaner, using a data loader:\n",
    "\n",
    "```python\n",
    "for xb,yb in train_dl:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 784)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7,\n",
       "       4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n",
       "       8, 3, 7, 7, 8, 4], dtype=int32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3, dtype=int32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(xb[0].view(28,28))\n",
    "plt.imshow(jnp.reshape(xb[0], (28,28)))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit():\n",
    "#     for epoch in range(epochs):\n",
    "#         for xb,yb in train_dl:\n",
    "#             preds = model(xb)\n",
    "#             loss = loss_func(preds, yb)\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "#             opt.zero_grad()\n",
    "#         report(loss, preds, yb)\n",
    "\n",
    "def fit(model, opt):\n",
    "    model, params = model\n",
    "    opt, opt_state = opt\n",
    "    jax_fw_pass = curry(get_loss_model, model)\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, n, bs):\n",
    "            s = slice(i, min(n,i+bs))\n",
    "            xb,yb = x_train[s],y_train[s]\n",
    "            (loss, preds), grads = jax.value_and_grad(jax_fw_pass, argnums=0, has_aux=True)(params, xb, yb)     \n",
    "            updates, opt_state = opt.update(grads, opt_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "        report(loss, preds, yb)\n",
    "\n",
    "    return model, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15, 0.96\n",
      "0.08, 0.98\n",
      "0.09, 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array([4.02369595e-04, 2.97556235e-03, 1.20990109e-04, 8.19942856e-04,\n",
       "        4.24507959e-03, 1.01011658e+00, 4.05613407e-02, 7.03583751e-03,\n",
       "        5.26778772e-03, 5.85871823e-02, 3.69541958e-05, 3.11683718e-04,\n",
       "        3.12323464e-05, 6.53768424e-04, 1.29762515e-02, 1.53114344e-03,\n",
       "        6.95344410e-04, 3.12911137e-03, 6.03180815e-05, 5.73351514e-04,\n",
       "        2.31725331e-02, 1.99401565e-02, 3.35939066e-03, 1.49055442e-03,\n",
       "        3.29974666e-03, 2.63593579e-03, 1.11931258e-04, 3.06279818e-03,\n",
       "        3.04929148e-02, 2.01189313e-02, 8.59487162e-04, 7.48354243e-04,\n",
       "        8.17798893e-04, 1.23363798e-02, 8.17742257e-05, 4.69949516e-03,\n",
       "        6.46093467e-05, 3.20059322e-02, 1.34544424e-03, 1.14791954e-04,\n",
       "        2.85140481e-02, 1.07024297e-01, 3.21864559e-06, 2.35965420e-02,\n",
       "        5.45951771e-03, 1.10981846e-03, 9.79221314e-02, 1.81589760e-02,\n",
       "        2.74581388e-02, 5.32841869e-02], dtype=float32),\n",
       " Array(0.97999996, dtype=float32))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit()\n",
    "# loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "\n",
    "model, params = fit(model, opt)\n",
    "loss_func(model.apply(params, xb), yb), accuracy(model.apply(params, xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle\n",
    "    def __iter__(self):\n",
    "        res = list(range(self.n))\n",
    "        if self.shuffle: random.shuffle(res)\n",
    "        return iter(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = Sampler(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "it = iter(ss)\n",
    "for o in range(5): print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48525, 41912, 24600, 14957, 8632]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Sampler(train_ds, shuffle=True)\n",
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler():\n",
    "    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n",
    "    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[41561, 8784, 24004, 46171],\n",
       " [18139, 26628, 33037, 44230],\n",
       " [16891, 34846, 10539, 19187],\n",
       " [33371, 48671, 29803, 10264],\n",
       " [15176, 18463, 6270, 48561]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchs = BatchSampler(ss, 4)\n",
    "list(islice(batchs, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return jnp.stack(xs),jnp.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n",
    "    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(Sampler(train_ds, shuffle=True ), bs)\n",
    "valid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batchs=train_samp)\n",
    "valid_dl = DataLoader(valid_ds, batchs=valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3, dtype=int32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "# plt.imshow(xb[0].view(28,28))\n",
    "plt.imshow(jnp.reshape(xb[0], (28,28)))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 784), (50,))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15, 0.96\n",
      "0.08, 0.98\n",
      "0.09, 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Model(\n",
       "     # attributes\n",
       "     nh = 50\n",
       " ),\n",
       " {'params': {'Dense_0': {'bias': Array([-0.15659213,  0.0802639 , -0.1906464 , -0.04901754, -0.5133901 ,\n",
       "           -0.08605969,  0.05876019,  0.04231644, -0.01707657,  0.15483388,\n",
       "            0.1236347 ,  0.3917885 ,  0.00687274, -0.20486538,  0.12158427,\n",
       "            0.5108256 ,  0.25512758, -0.27549705,  0.31419104,  0.04536387,\n",
       "           -0.0366312 , -0.03733613,  0.11489447,  0.54547477, -0.21809979,\n",
       "           -0.11544421, -0.09879699, -0.0822797 , -0.07752538, -0.07340132,\n",
       "           -0.5137772 , -0.41313404,  0.00544193,  0.06640723,  0.03280573,\n",
       "           -0.07977369,  0.04062178,  0.01304291, -0.03443436, -0.22928455,\n",
       "           -0.04526258,  0.3229372 ,  0.0637643 , -0.06092556, -0.04624177,\n",
       "           -0.10067464, -0.10254364,  0.4358636 ,  0.02602702, -0.17212023],      dtype=float32),\n",
       "    'kernel': Array([[-0.01414529, -0.00688224,  0.0536295 , ..., -0.00789963,\n",
       "            -0.03173989,  0.0042826 ],\n",
       "           [ 0.04734543,  0.01575986,  0.01513501, ...,  0.05555249,\n",
       "             0.06085142, -0.04095751],\n",
       "           [-0.06173804,  0.0276018 ,  0.06801958, ...,  0.00629877,\n",
       "            -0.01648072, -0.04200539],\n",
       "           ...,\n",
       "           [ 0.02190843, -0.00044944,  0.01136125, ...,  0.03082576,\n",
       "            -0.0154707 ,  0.0353416 ],\n",
       "           [ 0.05249991,  0.07788163, -0.03797574, ..., -0.01600541,\n",
       "            -0.04087879, -0.05345904],\n",
       "           [ 0.03012186, -0.01459131, -0.01418447, ...,  0.05843323,\n",
       "             0.00960819, -0.00700323]], dtype=float32)},\n",
       "   'Dense_1': {'bias': Array([-0.4256198 , -0.30362955, -0.33088386,  0.11481711,  0.12368941,\n",
       "            0.2489941 , -1.1790893 , -0.01194623,  1.1875638 ,  0.57610476],      dtype=float32),\n",
       "    'kernel': Array([[ 0.36098766, -0.21026117,  0.3799862 , -0.01339337,  0.29436356,\n",
       "             0.78169626,  0.00246325, -0.19720054, -0.9152441 ,  0.25379193],\n",
       "           [-0.10287388,  0.13225512,  0.39930326,  1.1034899 , -0.3093372 ,\n",
       "            -0.19556147, -0.4952391 ,  0.54586005, -0.4604696 , -0.33293083],\n",
       "           [ 0.10518772,  0.41910672, -0.7231537 , -0.65508837,  0.43602714,\n",
       "             0.4500928 ,  1.233503  , -0.06504973, -0.21988627, -0.15989226],\n",
       "           [ 0.08060192,  0.93600076, -0.34218454, -0.33932105,  0.39728013,\n",
       "             0.258624  , -0.05025655,  0.68738884, -0.74372554, -0.16331989],\n",
       "           [ 0.90759057, -0.12299787,  0.5559304 , -0.3550079 , -0.07242594,\n",
       "            -1.0093367 , -0.33291227,  0.25349095,  0.39141175,  0.6095624 ],\n",
       "           [-0.21908312, -0.38922733, -0.15389961,  0.1759922 ,  0.12497656,\n",
       "            -0.04330133, -0.47373095,  0.4216456 , -0.13818865,  0.21255936],\n",
       "           [-0.537606  , -0.30021715, -0.55642295,  1.1302661 , -0.33651134,\n",
       "             0.9991843 , -1.0152894 , -0.56887877, -0.53811073,  0.8425432 ],\n",
       "           [-0.45478967,  1.5034906 ,  0.46342823, -0.43778616, -0.1875188 ,\n",
       "            -0.00304477, -0.03475302,  0.01190577, -0.42113796,  0.13397272],\n",
       "           [-0.17926615, -0.11838992, -0.03155185,  0.20762746, -0.05222336,\n",
       "            -0.36942136, -0.04883825,  0.12616384, -0.19718555,  0.03090904],\n",
       "           [-0.00326971,  0.13181117,  1.0750607 ,  0.14566727,  0.69787383,\n",
       "            -0.03397791, -0.57767886, -0.13193241, -0.2911906 , -0.87992054],\n",
       "           [-0.16776754, -0.06770018,  0.56857026,  0.01771662,  0.5693234 ,\n",
       "            -0.56525505, -0.0458337 ,  0.81201935, -0.6031476 , -0.58583677],\n",
       "           [-0.10198459, -0.53744566,  0.15161905,  0.607717  , -0.519191  ,\n",
       "             0.10170529, -0.9813174 ,  0.9338426 , -0.36958617,  0.61043143],\n",
       "           [-0.3522778 ,  0.3875822 , -0.5237344 , -0.05966273,  0.2002601 ,\n",
       "            -0.29790148,  0.09864619,  0.8367988 , -0.7265284 ,  0.5419112 ],\n",
       "           [-0.19151004, -0.29742423,  0.3667551 ,  0.8044683 , -0.17927933,\n",
       "            -0.14998095, -0.2960789 , -0.18939073, -0.45756117, -0.72565854],\n",
       "           [-0.14860433, -0.23298071,  0.275013  , -0.1577427 ,  0.12307227,\n",
       "             0.47444853, -0.2052962 , -0.38210845, -0.17656742, -0.27851978],\n",
       "           [-0.33008584,  0.02450478, -0.9855271 , -0.67126733,  0.29607508,\n",
       "             0.83982015, -0.16750097,  0.8647985 , -0.2360814 , -0.31282958],\n",
       "           [-0.36229345,  0.72889274,  0.8196858 , -0.37234795,  0.11221453,\n",
       "            -0.16419816,  0.11561252,  1.1759777 , -0.21250452, -0.7011717 ],\n",
       "           [ 0.9038696 , -0.47870362,  0.02403911, -0.05423765,  0.29834068,\n",
       "            -0.3240407 ,  0.3980104 , -0.60051286, -0.01991564, -0.6590198 ],\n",
       "           [-0.7049704 ,  0.1917097 , -0.0315793 ,  0.66917384, -1.2692506 ,\n",
       "             1.0550811 ,  0.48984057, -0.15652688,  0.2571561 , -0.50980073],\n",
       "           [-0.31500936, -0.5192521 ,  0.09108265, -0.41916624,  1.0008882 ,\n",
       "            -0.23181468,  0.02194471, -0.10821667, -0.7517731 ,  0.8526354 ],\n",
       "           [ 1.0940912 , -0.17298183, -0.45967698, -0.392762  ,  0.42963699,\n",
       "            -0.04470652,  0.5475363 ,  0.03649717, -0.90140295,  0.26384756],\n",
       "           [ 0.08009673, -0.28048053, -0.15472463,  0.3485711 , -0.20696726,\n",
       "            -0.03997078, -0.15008584,  0.5968148 , -0.19687515,  0.15596223],\n",
       "           [-0.55543   ,  0.53979903, -0.6425236 , -0.20768906,  0.9669329 ,\n",
       "            -0.24579892, -0.6311858 ,  0.2869077 , -1.0210954 ,  0.91539896],\n",
       "           [ 0.45509145, -0.35794678, -0.13279217, -0.4252084 , -1.1455911 ,\n",
       "             0.73823607,  0.64450073,  0.0826879 , -0.28204897, -0.56905293],\n",
       "           [-0.18509658,  0.33783972,  0.02681776, -0.07698444, -0.07678935,\n",
       "             0.22492707,  0.03693265,  0.05074019, -0.65815884,  0.36538792],\n",
       "           [-0.12037099,  0.16419744, -0.1039077 ,  0.16046348, -0.12231456,\n",
       "            -0.45482588, -0.38783813,  0.3463786 , -0.18685414,  0.3321155 ],\n",
       "           [-0.3083577 ,  0.02688666,  0.15570632,  0.10742545, -0.07466039,\n",
       "            -0.14222822,  0.02813946, -0.1292668 ,  0.14667201, -0.14335275],\n",
       "           [ 0.11841981,  0.0740526 , -0.15332438,  0.00879284,  0.07943912,\n",
       "            -0.09442446,  0.32443658,  0.00318264, -0.17958784,  0.3385334 ],\n",
       "           [-0.132061  ,  0.2989437 ,  0.1461703 ,  0.15122445, -0.09537779,\n",
       "            -0.2810953 , -0.07878176, -0.01774848, -0.22709902,  0.24716607],\n",
       "           [-0.14612533, -0.17746316, -0.27814022, -0.3360445 ,  0.14601594,\n",
       "             0.00989044,  0.2319116 ,  0.34544486, -0.41578877,  0.288981  ],\n",
       "           [-0.13546132,  0.28629348,  0.08827848,  0.38094458, -0.4111328 ,\n",
       "            -0.66156805, -0.37480247,  0.026832  ,  0.6432515 , -0.07594929],\n",
       "           [ 0.20822008, -0.51270336,  0.18788591, -0.21410191, -0.5667185 ,\n",
       "            -0.21540137, -0.22814688, -0.12816128,  0.32599166,  0.5209168 ],\n",
       "           [-0.11229997,  0.18922146, -0.03782133, -0.03524672, -0.03794118,\n",
       "            -0.0833928 ,  0.13749987,  0.13225569, -0.01053631,  0.1155801 ],\n",
       "           [-0.34002405,  1.0947579 ,  0.3224026 , -0.3971637 , -0.19520798,\n",
       "             0.12528102, -0.24857897, -0.36451825, -0.77432513, -0.10272685],\n",
       "           [ 0.58973765, -0.21102534,  0.48096803,  0.05156507,  0.06658422,\n",
       "             0.14924297, -0.24234615, -0.40452406, -0.73934823,  0.19249988],\n",
       "           [-0.63972086, -0.6183107 , -0.8290669 ,  0.48008922,  0.19466446,\n",
       "             0.51327777, -0.5927152 , -0.22721532,  0.16584095,  0.5700164 ],\n",
       "           [ 0.05206207, -0.5042486 , -0.01100664, -0.66887385,  0.29953924,\n",
       "            -0.13614506,  0.92559576,  0.13530743,  0.48696086, -0.49213496],\n",
       "           [-0.09000197,  0.2545429 ,  0.29140073,  0.21150447, -0.33751693,\n",
       "             0.39281496,  0.2991888 , -0.63589865,  0.56100583, -0.70575947],\n",
       "           [-0.27758557,  0.3275639 ,  0.16046369, -0.02026597,  0.41665253,\n",
       "             0.16905893,  0.35997608, -0.01095998, -0.37238812, -0.19176681],\n",
       "           [ 0.14351851, -0.32714236, -0.26804808, -0.13654853,  0.36605456,\n",
       "            -0.2312797 ,  0.96645355, -0.36322805, -0.22935842, -0.29883817],\n",
       "           [ 0.31055033,  0.10172056, -0.4148936 , -0.6532558 , -0.09966889,\n",
       "             0.01309453,  0.04768926, -0.0834602 ,  0.17241462,  0.30073658],\n",
       "           [-0.5715548 ,  0.23673467,  1.0108162 ,  0.08531469, -0.29370382,\n",
       "            -0.06438767,  0.43924713, -0.08235309, -0.06217473, -0.90480655],\n",
       "           [ 0.00403773,  0.3120611 ,  0.53780675, -0.20210242,  0.25536874,\n",
       "             0.25698414, -0.1195587 , -0.4717442 , -0.29499146, -0.03681391],\n",
       "           [ 0.2510682 , -0.21293566, -0.0434441 ,  0.10195962, -0.21369265,\n",
       "             0.18785775,  0.05914693, -0.21647072, -0.05311317, -0.04976815],\n",
       "           [ 0.04256549,  0.31804794,  1.0344926 ,  0.67984855, -0.69697905,\n",
       "            -0.15627934, -0.6071752 ,  0.9637056 , -0.34706038, -1.1752921 ],\n",
       "           [-0.07972959, -0.02135135,  0.24119398,  0.10500741,  0.04967288,\n",
       "             0.17131601,  0.28900376, -0.12247148, -0.29005402, -0.04733602],\n",
       "           [ 0.17445557,  0.25211224,  0.02281875,  0.12159467, -0.20933041,\n",
       "             0.3451277 ,  0.2476176 , -0.18199395, -0.4731509 ,  0.15917107],\n",
       "           [ 0.18656254, -0.33517647, -1.0014958 , -0.27884158, -1.0016239 ,\n",
       "             1.1401805 ,  0.28324035,  0.09194275,  0.1620097 , -0.11931827],\n",
       "           [ 0.39653334, -0.4981756 , -0.3971587 ,  0.15020087, -0.65018463,\n",
       "             0.10318844,  0.34281576, -0.56229746, -0.28932768,  0.69580185],\n",
       "           [-0.2755528 , -0.04344565, -0.44128564, -0.727767  ,  1.0372458 ,\n",
       "             0.00284881, -0.29282016,  0.25063467,  0.75516033,  0.1461131 ]],      dtype=float32)}}})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from fastcore.basics import store_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " Array([1, 1, 1, 0], dtype=int32))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[jnp.array([3,6,8,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " Array([1, 1, 1, 0], dtype=int32))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__getitem__(jnp.array([3,6,8,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), Array([1, 1], dtype=int32))\n",
      "(Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), Array([1, 0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "for o in map(train_ds.__getitem__, (jnp.array([[3,6],[8,1]]))): print(o)\n",
    "# list(map(lambda o: train_ds.__getitem__(o), [3, 6, 8, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cytoolz.curried import compose_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()\n",
    "    def __iter__(self):\n",
    "        with mp.Pool(self.n_workers) as ex: yield from ex.map(\n",
    "            compose_left(\n",
    "                jnp.array,\n",
    "                self.ds.__getitem__, \n",
    "            ),\n",
    "            iter(self.batchs)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batchs=train_samp, n_workers=2)\n",
    "it = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell crashes in jax because jax already multithreaded\n",
    "# xb,yb = next(it)\n",
    "# xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(RandomSampler(train_ds),     bs, drop_last=False)\n",
    "valid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15, 0.96\n",
      "0.08, 0.98\n",
      "0.09, 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array([4.02369595e-04, 2.97556235e-03, 1.20990109e-04, 8.19942856e-04,\n",
       "        4.24507959e-03, 1.01011658e+00, 4.05613407e-02, 7.03583751e-03,\n",
       "        5.26778772e-03, 5.85871823e-02, 3.69541958e-05, 3.11683718e-04,\n",
       "        3.12323464e-05, 6.53768424e-04, 1.29762515e-02, 1.53114344e-03,\n",
       "        6.95344410e-04, 3.12911137e-03, 6.03180815e-05, 5.73351514e-04,\n",
       "        2.31725331e-02, 1.99401565e-02, 3.35939066e-03, 1.49055442e-03,\n",
       "        3.29974666e-03, 2.63593579e-03, 1.11931258e-04, 3.06279818e-03,\n",
       "        3.04929148e-02, 2.01189313e-02, 8.59487162e-04, 7.48354243e-04,\n",
       "        8.17798893e-04, 1.23363798e-02, 8.17742257e-05, 4.69949516e-03,\n",
       "        6.46093467e-05, 3.20059322e-02, 1.34544424e-03, 1.14791954e-04,\n",
       "        2.85140481e-02, 1.07024297e-01, 3.21864559e-06, 2.35965420e-02,\n",
       "        5.45951771e-03, 1.10981846e-03, 9.79221314e-02, 1.81589760e-02,\n",
       "        2.74581388e-02, 5.32841869e-02], dtype=float32),\n",
       " Array(0.97999996, dtype=float32))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "# fit()\n",
    "# loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "\n",
    "model, params = fit(model, opt)\n",
    "loss_func(model.apply(params, xb), yb), accuracy(model.apply(params, xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can auto-generate the BatchSampler for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can also generate the Sequential/RandomSamplers too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NO MULTIPROCESSING ATOP JAX!\n",
    "# train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\n",
    "# valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15, 0.96\n",
      "0.08, 0.98\n",
      "0.09, 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array([4.02369595e-04, 2.97556235e-03, 1.20990109e-04, 8.19942856e-04,\n",
       "        4.24507959e-03, 1.01011658e+00, 4.05613407e-02, 7.03583751e-03,\n",
       "        5.26778772e-03, 5.85871823e-02, 3.69541958e-05, 3.11683718e-04,\n",
       "        3.12323464e-05, 6.53768424e-04, 1.29762515e-02, 1.53114344e-03,\n",
       "        6.95344410e-04, 3.12911137e-03, 6.03180815e-05, 5.73351514e-04,\n",
       "        2.31725331e-02, 1.99401565e-02, 3.35939066e-03, 1.49055442e-03,\n",
       "        3.29974666e-03, 2.63593579e-03, 1.11931258e-04, 3.06279818e-03,\n",
       "        3.04929148e-02, 2.01189313e-02, 8.59487162e-04, 7.48354243e-04,\n",
       "        8.17798893e-04, 1.23363798e-02, 8.17742257e-05, 4.69949516e-03,\n",
       "        6.46093467e-05, 3.20059322e-02, 1.34544424e-03, 1.14791954e-04,\n",
       "        2.85140481e-02, 1.07024297e-01, 3.21864559e-06, 2.35965420e-02,\n",
       "        5.45951771e-03, 1.10981846e-03, 9.79221314e-02, 1.81589760e-02,\n",
       "        2.74581388e-02, 5.32841869e-02], dtype=float32),\n",
       " Array(0.97999996, dtype=float32))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "# fit()\n",
    "\n",
    "# loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "\n",
    "model, params = fit(model, opt)\n",
    "loss_func(model.apply(params, xb), yb), accuracy(model.apply(params, xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset actually already knows how to sample a batch of indices all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " Array([9, 1, 3], dtype=int32))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_ds[[4,6,7]]\n",
    "train_ds[jnp.array([4,6,7])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...that means that we can actually skip the batch_sampler and collate_fn entirely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not working in jax\n",
    "train_dl = DataLoader(train_ds, sampler=train_samp)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not working - something in torch not doing the array lookup over a jnp.array of idx\n",
    "# xb,yb = next(iter(train_dl))\n",
    "# xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         for xb,yb in train_dl:\n",
    "#             loss = loss_func(model(xb), yb)\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "#             opt.zero_grad()\n",
    "\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tot_loss,tot_acc,count = 0.,0.,0\n",
    "#             for xb,yb in valid_dl:\n",
    "#                 pred = model(xb)\n",
    "#                 n = len(xb)\n",
    "#                 count += n\n",
    "#                 tot_loss += loss_func(pred,yb).item()*n\n",
    "#                 tot_acc  += accuracy (pred,yb).item()*n\n",
    "#         print(epoch, tot_loss/count, tot_acc/count)\n",
    "#     return tot_loss/count, tot_acc/count\n",
    "\n",
    "\n",
    "def get_loss_model(loss_func, model, params, features, y_true):\n",
    "    preds = model.apply(params, features)\n",
    "    loss = jnp.mean(loss_func(preds, y_true))\n",
    "    return loss, preds\n",
    "    \n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    model, params = model\n",
    "    opt, opt_state = opt\n",
    "    jax_fw_pass = curry(get_loss_model, loss_func, model)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            (loss, preds), grads = jax.value_and_grad(jax_fw_pass, argnums=0, has_aux=True)(params, xb, yb)     \n",
    "            updates, opt_state = opt.update(grads, opt_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "        tot_loss, tot_acc, count = 0.,0.,0\n",
    "        for xb, yb in valid_dl:\n",
    "            pred = model.apply(params, xb)\n",
    "            n = len(xb)\n",
    "            count += n\n",
    "            tot_loss += jnp.mean(loss_func(pred, yb))*n\n",
    "            tot_acc += accuracy(pred, yb)*n\n",
    "        print(epoch, tot_loss/count, tot_acc/count)\n",
    "\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, bs, shuffle=True, collate_fn=collate, **kwargs),\n",
    "            DataLoader(valid_ds, bs*2, collate_fn=collate, **kwargs))\n",
    "\n",
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return jnp.stack(xs),jnp.stack(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.14854692 0.9549\n"
     ]
    }
   ],
   "source": [
    "%time loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
