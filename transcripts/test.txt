Hi all and welcome to Lesson 15. And what we're going to endeavor to do today is to  create a convolutional autoencoder. And in the process, we will see why  doing that well is a tricky thing to do and time permitting, we will begin to work on a  framework, a deep learning framework to make life a lot easier. Not sure  how far we'll get on that today time wise. So let's see how we go and get straight into it. Okay. So today, let's start by talking before we can create a convolutional autoencoder,  we know to talk about convolutions and what are they and what are they for. Broadly speaking,  convolutions are something that allows us to, to tell our neural network a little bit about the  structure of the problem. That's going to make it a lot easier for it to solve the problem. And in particular, the structure of our problem is we're doing things with images. Images are  laid out on a grid, a 2d grid for black and white or a 3d for color or a 4d for a color video or  whatever. And so we would say, you know, there's a relationship between the pixels going across  and the pixels going down. They tend to be similar to each other, differences in those pixels across  those dimensions tend to have meaning. Sets, patterns of pixels that appear in different  places often represent the same thing. So for example, a cat in the top left is still a cat,  even if it's in the bottom right. These kinds of, this kind of prior information is something  that is naturally captured by a Convolutional Neural Network, something that uses convolutions. Generally speaking, this is a good thing because it means that we will be able to  use less parameters and less computation because more of that information about the problem we're  solving is kind of encoded directly into our architecture. There are other architectures that  don't encode that prior information as strongly, such as a Multi-Layer Perceptron, which we've been  looking at so far, or a Transformers network, which we haven't looked at yet. Those kinds of  architectures could potentially give us, or they do give us more flexibility and given enough time,  compute and data, they could potentially find things that maybe CNNs would struggle to find. So we're not always going to use Convolutional Neural Networks,  but they're a pretty good starting point and certainly something important to understand.  They're not just used for images. We can also take advantage of one-dimensional convolutions  for language-based tasks, for instance. So convolutions come up a lot. So in this notebook, one thing you'll notice that might be of interest is we  are importing stuff from miniai now. Now miniai is this little library that we're  starting to create and we're creating it using nbdev. So we've got a miniai.training  and a miniai.datasets. And so if we look, for example, at the datasets notebook,  it starts with something that says that the default export is called datasets. And some  of the cells have a export directive on them.
